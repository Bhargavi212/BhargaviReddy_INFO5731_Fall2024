{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The research question that I considered is \"How do the publication trends in artificial intelligence (AI) differ between various newspapers?\"\n",
        "The data collected in this is Title, content, publication date, author, and source (news outlet).\n",
        "Articles specifically focused on artificial intelligence (AI), its technologies, ethics, impact on society, and industries\n",
        "\n",
        "The amount of data required for this is upto 1000to 2000 samples of data.\n",
        "The steps used for collecting and saving data is:\n",
        "1. Identify the data sources\n",
        "2. Define the query required\n",
        "3. Define the data collection plan that may be done by using various methods that may be beautiful soup etc.,\n",
        "4. Data storage and saving of the data.\n",
        "\n",
        "the data is generally stored in the csv files as required.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-moc9Hvane9I",
        "outputId": "da55059b-91af-40e2-df22-e4f4d416210d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The research question that I considered is \"How do the publication trends in artificial intelligence (AI) differ between various newspapers?\"\\nThe data collected in this is Title, content, publication date, author, and source (news outlet).\\nArticles specifically focused on artificial intelligence (AI), its technologies, ethics, impact on society, and industries\\n\\nThe amount of data required for this is upto 1000to 2000 samples of data.\\nThe steps used for collecting and saving data is:\\n1. Identify the data sources\\n2. Define the query required\\n3. Define the data collection plan that may be done by using various methods that may be beautiful soup etc.,\\n4. Data storage and saving of the data.\\n\\nthe data is generally stored in the csv files as required.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "base_url = 'https://timesofindia.indiatimes.com/'\n",
        "search_query = 'artificial intelligence'\n",
        "total= 1000\n",
        "a_collected = 0\n",
        "a_list = []\n",
        "\n",
        "# Keywords to track\n",
        "kwords = ['AI', 'ethics', 'machine learning', 'automation']\n",
        "\n",
        "def fetch_article_links(search_url, num_pages):\n",
        "    article_links = []\n",
        "    for page in range(1, num_pages + 1):\n",
        "        params = {'q': search_query, 'page': page}\n",
        "        response = requests.get(search_url, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Find article links (modify selector based on actual site)\n",
        "            links = soup.find_all('a', href=True)\n",
        "            for link in links:\n",
        "                href = link['href']\n",
        "                if 'article' in href:\n",
        "                    full_url = href if href.startswith('http') else f\"https://timesofindia.indiatimes.com{href}\"\n",
        "                    article_links.append(full_url)\n",
        "        else:\n",
        "            print(f\"Failed to retrieve page {page}. Status code: {response.status_code}\")\n",
        "\n",
        "    return article_links\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"Returns sentiment polarity: positive, negative, or neutral\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0:\n",
        "        return 'positive'\n",
        "    elif polarity < 0:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "def count_keywords(text, keywords):\n",
        "    \"\"\"Returns a dictionary of keyword frequencies in the text\"\"\"\n",
        "    word_list = text.lower().split()\n",
        "    word_counts = Counter(word_list)\n",
        "    keyword_counts = {keyword: word_counts.get(keyword.lower(), 0) for keyword in keywords}\n",
        "    return keyword_counts\n",
        "\n",
        "def scrape_article(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            title = soup.find('h1').get_text() if soup.find('h1') else 'No Title'\n",
        "            date = soup.find('time')['datetime'] if soup.find('time') else 'No Date'\n",
        "            content = soup.find('div', class_='article-body').get_text() if soup.find('div', class_='article-body') else 'No Content'\n",
        "            sentiment = analyze_sentiment(content)\n",
        "            keyword_freq = count_keywords(content, kwords)\n",
        "\n",
        "            return {'title': title, 'date': date, 'content': content, 'url': url, 'sentiment': sentiment, **keyword_freq}\n",
        "        else:\n",
        "            print(f\"Failed to retrieve article at {url}. Status code: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching article {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    global articles_collected\n",
        "    num_pages = 10  # Number of search result pages to scrape\n",
        "\n",
        "    print(\"Fetching article links...\")\n",
        "    article_links = fetch_article_links(base_url, num_pages)\n",
        "    print(f\"Found {len(article_links)} article links.\")\n",
        "\n",
        "    print(\"Scraping articles...\")\n",
        "    for link in article_links:\n",
        "        if articles_collected >= total_articles:\n",
        "            break\n",
        "\n",
        "        article = scrape_article(link)\n",
        "        if article:\n",
        "            a_list.append(article)\n",
        "            articles_collected += 1\n",
        "            print(f\"Collected {articles_collected}/{total_articles} articles\")\n",
        "\n",
        "        time.sleep(1)  # Be respectful and avoid overwhelming the server\n",
        "\n",
        "    print(\"Saving data...\")\n",
        "    df = pd.DataFrame(a_list)\n",
        "    df.to_csv('articles.csv', index=False)\n",
        "    print(f\"Data saved to articles.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ikzkCKC1k0",
        "outputId": "9e5e32a1-8d33-4ec8-f0da-ba1668be95e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching article links...\n",
            "Found 2310 article links.\n",
            "Scraping articles...\n",
            "Saving data...\n",
            "Data saved to articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 requests\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da25b98-a564-4efb-a81c-de52be222120"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def fetch_academic_news(query, start_year, end_year, total_articles):\n",
        "    articles = []\n",
        "    base_url = 'https://www.semanticscholar.org/blog'\n",
        "    offset = 0\n",
        "    rows_per_request = 10  # Adjust based on the website's pagination\n",
        "\n",
        "    while len(articles) < total_articles:\n",
        "        response = requests.get(base_url, params={'query': query, 'start': offset, 'limit': rows_per_request})\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        news_items = soup.find_all('div', class_='post-item')  # Update the selector based on actual HTML\n",
        "\n",
        "        for item in news_items:\n",
        "            title = item.find('h2', class_='title').text.strip() if item.find('h2', class_='title') else 'N/A'\n",
        "            date_str = item.find('time', class_='date').text.strip() if item.find('time', class_='date') else 'N/A'\n",
        "            try:\n",
        "                date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
        "                year = date_obj.year\n",
        "            except ValueError:\n",
        "                year = 'N/A'\n",
        "\n",
        "            if year == 'N/A' or int(year) < start_year or int(year) > end_year:\n",
        "                continue\n",
        "\n",
        "            authors = item.find('span', class_='authors').text.strip() if item.find('span', class_='authors') else 'N/A'\n",
        "            abstract = item.find('p', class_='abstract').text.strip() if item.find('p', class_='abstract') else 'N/A'\n",
        "            venue = item.find('span', class_='venue').text.strip() if item.find('span', class_='venue') else 'N/A'\n",
        "\n",
        "            articles.append([title, venue, year, authors, abstract])\n",
        "\n",
        "            if len(articles) >= total_articles:\n",
        "                break\n",
        "\n",
        "        offset += rows_per_request\n",
        "        if len(news_items) < rows_per_request:\n",
        "            break  # Exit loop if no more items are returned\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Parameters for scraping\n",
        "query = \"artificial intelligence\"\n",
        "start_year = 2014\n",
        "end_year = 2024\n",
        "total_articles = 1000\n",
        "\n",
        "# Fetch articles\n",
        "articles = fetch_academic_news(query, start_year, end_year, total_articles)\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(articles, columns=['Title', 'Venue', 'Year', 'Authors', 'Abstract'])\n",
        "df.to_csv(\"academic_ai_news_articles.csv\", index=False)\n",
        "\n",
        "print(\"Data collection completed. Articles are saved in 'academic_ai_news_articles.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHp7Q2i03HAf",
        "outputId": "6ae7471a-c47d-4a0a-8104-d7dc4e20b43b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection completed. Articles are saved in 'academic_ai_news_articles.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d6aefa-85e4-4a8e-f0cf-c38c10bafbe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n",
            "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the Reddit API client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='H2pzWfXFrWmjPSy63_8z3A',\n",
        "    client_secret='SBHhAkSF7xn8G2miJrwlNsy3PrQWvg',\n",
        "    user_agent='Siva'\n",
        ")\n",
        "# Function to collect data from Reddit\n",
        "def collect_reddit_data(subreddit_name, keyword, limit=100):\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    posts = subreddit.search(keyword, limit=limit)\n",
        "\n",
        "    data = []\n",
        "    for post in posts:\n",
        "        data.append({\n",
        "            'title': post.title,\n",
        "            'score': post.score,\n",
        "            'id': post.id,\n",
        "            'url': post.url,\n",
        "            'created': post.created_utc,\n",
        "            'num_comments': post.num_comments\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Collect data\n",
        "df_reddit = collect_reddit_data('python', 'data science', 10)\n",
        "print(df_reddit)\n",
        "\n",
        "# Save to a CSV file\n",
        "df_reddit.to_csv('reddit_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eOdlivqzhbR",
        "outputId": "58c6f55d-6a17-479c-9d5d-9f284a4d19b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  score       id  \\\n",
            "0  I teach Python courses - here's my collection ...   2969   jii8ex   \n",
            "1  Six months into Python and Data science, my fi...   1731   g5ymoy   \n",
            "2  Modern alternatives to Data Science Libraries ...    211  196jbms   \n",
            "3  If you're a beginner interested in data scienc...    827   xyyj9t   \n",
            "4  If you're a beginner interested in data scienc...    701  12j68f7   \n",
            "5  1 year ago I started building Practice Probs -...    782   zzv4zt   \n",
            "6  Python Data Science December [Completed] - 24 ...    517   zu7vqp   \n",
            "7  78 Python data science practice problems in a ...    777   u77fce   \n",
            "8  Build a Data Science SaaS App with Just Python...    102  173qcwe   \n",
            "9  What features of the Python language predestin...     78   zu8azk   \n",
            "\n",
            "                                                 url       created  \\\n",
            "0      https://marko-knoebl.github.io/slides/#python  1.603731e+09   \n",
            "1                    https://v.redd.it/mlqov8dbicu41  1.587551e+09   \n",
            "2  https://www.reddit.com/r/Python/comments/196jb...  1.705249e+09   \n",
            "3  https://youtube.com/playlist?list=PLvICEeb-TZE...  1.665250e+09   \n",
            "4  https://www.youtube.com/playlist?list=PLvICEeb...  1.681265e+09   \n",
            "5  https://www.reddit.com/r/Python/comments/zzv4z...  1.672497e+09   \n",
            "6  https://www.reddit.com/r/Python/comments/zu7vq...  1.671881e+09   \n",
            "7       https://github.com/practiceprobs/problemsets  1.650381e+09   \n",
            "8  https://www.reddit.com/r/Python/comments/173qc...  1.696854e+09   \n",
            "9  https://www.reddit.com/r/Python/comments/zu8az...  1.671883e+09   \n",
            "\n",
            "   num_comments  \n",
            "0            88  \n",
            "1           139  \n",
            "2            69  \n",
            "3            32  \n",
            "4            24  \n",
            "5            26  \n",
            "6            37  \n",
            "7            22  \n",
            "8            43  \n",
            "9            78  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Web Scrapping is very new for me.I may need more detailed explaination if you could\n",
        "explain more detailed that would be very helpful. It is very great learning new things with each other. Thank you.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}