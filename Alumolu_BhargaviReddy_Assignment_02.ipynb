{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "outputId": "ac636113-b599-4223-b174-48811f0fb993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_movie_reviews(movie_id, max_reviews=1000):\n",
        "    reviews_list = []\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    page_number = 1\n",
        "    review_count = 0\n",
        "\n",
        "    while review_count < max_reviews:\n",
        "        # IMDb user review page pattern\n",
        "        url = f'https://www.imdb.com/title/tt{movie_id}/reviews?ref_=nmawd_awd_1&start={page_number * 10}'\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve reviews from {url}\")\n",
        "            break\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Parsing review content\n",
        "        review_blocks = soup.find_all('div', class_='review-container')\n",
        "\n",
        "        for review_block in review_blocks:\n",
        "            review_data = {}\n",
        "\n",
        "            # Get the review title\n",
        "            title_tag = review_block.find('a', class_='title')\n",
        "            review_data['title'] = title_tag.text.strip() if title_tag else 'No Title'\n",
        "\n",
        "            # Get the rating\n",
        "            rating_tag = review_block.find('span', class_='rating-other-user-rating')\n",
        "            review_data['rating'] = rating_tag.span.text.strip() if rating_tag else 'No Rating'\n",
        "\n",
        "            # Get the full review text\n",
        "            review_text_tag = review_block.find('div', class_='text show-more__control')\n",
        "            review_data['review'] = review_text_tag.text.strip() if review_text_tag else 'No Review Text'\n",
        "\n",
        "            # Add to the list\n",
        "            reviews_list.append(review_data)\n",
        "            review_count += 1\n",
        "\n",
        "            if review_count >= max_reviews:\n",
        "                break\n",
        "\n",
        "        # Check if there are more pages to scrape\n",
        "        if len(review_blocks) < 10:  # If less than 10 reviews were found, stop scraping\n",
        "            break\n",
        "\n",
        "        page_number += 1\n",
        "\n",
        "    return reviews_list\n",
        "\n",
        "def save_reviews_to_csv(reviews, movie_name):\n",
        "    df = pd.DataFrame(reviews)\n",
        "    df.to_csv(f'{movie_name}_imdb_reviews.csv', index=False)\n",
        "    print(f\"Saved {len(reviews)} reviews to {movie_name}_imdb_reviews.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Movie IDs for the selected movies\n",
        "    movies = [\n",
        "        {\"title\": \"Bahubali\", \"id\": \"4849438\"},\n",
        "        {\"title\": \"RRR\", \"id\": \"8178634\"},\n",
        "        {\"title\": \"Salaar\", \"id\": \"13927994\"}\n",
        "    ]\n",
        "\n",
        "    for movie in movies:\n",
        "        print(f\"Collecting reviews for: {movie['title']}\")\n",
        "        reviews = get_movie_reviews(movie['id'])\n",
        "        save_reviews_to_csv(reviews, movie['title'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VW-x3XF9_bX3",
        "outputId": "e6598fc2-19e7-4b36-a8fa-d48bdcca0013"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reviews for: Bahubali\n",
            "Saved 50 reviews to Bahubali_imdb_reviews.csv\n",
            "Collecting reviews for: RRR\n",
            "Saved 100 reviews to RRR_imdb_reviews.csv\n",
            "Collecting reviews for: Salaar\n",
            "Saved 50 reviews to Salaar_imdb_reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file names of the CSV files to be combined\n",
        "files = [\n",
        "    'Bahubali_imdb_reviews.csv',\n",
        "    'RRR_imdb_reviews.csv',\n",
        "    'Salaar_imdb_reviews.csv'\n",
        "]\n",
        "\n",
        "# Initialize an empty list to hold the DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Loop through each file and read it into a DataFrame\n",
        "for file in files:\n",
        "    try:\n",
        "        # Read the CSV file into a DataFrame\n",
        "        df = pd.read_csv(file)\n",
        "        # Add a column for the movie name extracted from the file name\n",
        "        df['movie'] = file.split('_')[0]  # Extracting movie name from the file name\n",
        "        # Append the DataFrame to the list\n",
        "        dataframes.append(df)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {file} not found. Please check the file path.\")\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('combined_imdb_reviews.csv', index=False)\n",
        "\n",
        "print(f\"Combined reviews saved to 'combined_imdb_reviews.csv' with {len(combined_df)} reviews.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw9akzYBBiwx",
        "outputId": "f6a47103-9651-461e-dea7-3eabd22fc317"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined reviews saved to 'combined_imdb_reviews.csv' with 200 reviews.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0f93cd-dc49-4e23-f2e2-4e17af205815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas nltk\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file containing the reviews\n",
        "df = pd.read_csv('combined_imdb_reviews.csv')\n",
        "\n",
        "# Ensure the review column exists\n",
        "if 'review' not in df.columns:\n",
        "    raise ValueError(\"The 'review' column was not found in the CSV file.\")\n",
        "\n",
        "# Initialize objects for stemming and lemmatization\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean the text data\n",
        "def clean_text(text):\n",
        "    # (1) Remove noise, such as special characters and punctuations\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "    # (2) Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # (3) Tokenize and remove stopwords\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # (4) Lowercase all text\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # (5) Stemming\n",
        "    words_stemmed = [ps.stem(word) for word in words]\n",
        "\n",
        "    # (6) Lemmatization (after stemming for comparison)\n",
        "    words_lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words), ' '.join(words_stemmed), ' '.join(words_lemmatized)\n",
        "\n",
        "# Apply the clean_text function to the review column and save to new columns\n",
        "df['clean_text'], df['stemmed_text'], df['lemmatized_text'] = zip(*df['review'].apply(clean_text))\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('combined_imdb_reviews_cleaned.csv', index=False)\n",
        "\n",
        "print(\"Cleaned data has been saved to 'combined_imdb_reviews_cleaned.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8fWSdg54ul0",
        "outputId": "db97c4db-c429-4103-b8e6-accca1d795aa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data has been saved to 'combined_imdb_reviews_cleaned.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7416fa-147b-460e-f023-8a12755e85ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "# Load the spaCy model for parsing and named entity recognition\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the cleaned data\n",
        "df = pd.read_csv(\"combined_imdb_reviews_cleaned.csv\")\n",
        "\n",
        "# Ensure the clean text column exists\n",
        "if 'clean_text' not in df.columns:\n",
        "    raise ValueError(\"The 'clean_text' column was not found in the CSV file.\")\n",
        "\n",
        "# Initialize NLTK POS tagger\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function to conduct POS tagging and count specific POS tags\n",
        "def pos_tagging_and_count(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Count nouns, verbs, adjectives, and adverbs\n",
        "    pos_count = Counter([tag for word, tag in pos_tags])\n",
        "\n",
        "    # Calculate total for Nouns (NN, NNS, NNP, NNPS), Verbs (VB, VBD, VBG, etc.)\n",
        "    noun_count = sum(pos_count[tag] for tag in ['NN', 'NNS', 'NNP', 'NNPS'])\n",
        "    verb_count = sum(pos_count[tag] for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
        "    adj_count = sum(pos_count[tag] for tag in ['JJ', 'JJR', 'JJS'])\n",
        "    adv_count = sum(pos_count[tag] for tag in ['RB', 'RBR', 'RBS'])\n",
        "\n",
        "    return pos_tags, noun_count, verb_count, adj_count, adv_count\n",
        "\n",
        "# Function to perform constituency parsing and dependency parsing\n",
        "def parse_sentence(sentence):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Dependency parsing\n",
        "    print(\"\\nDependency Parsing Tree:\")\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} -> {token.dep_} (Head: {token.head.text})\")\n",
        "\n",
        "    # Constituency parsing can be visualized using external tools like Berkeley Neural Parser, but Spacy doesn't have a built-in constituency parser.\n",
        "\n",
        "# Function to perform Named Entity Recognition (NER)\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Count entities\n",
        "    entity_count = Counter([ent.label_ for ent in doc.ents])\n",
        "\n",
        "    return entities, entity_count\n",
        "\n",
        "# Iterate through the reviews and perform analysis on the first review for demo purposes\n",
        "for i, row in df.iterrows():\n",
        "    clean_text = row['clean_text']\n",
        "\n",
        "    print(f\"\\n### Review {i+1} ###\")\n",
        "    print(f\"Clean Text: {clean_text}\")\n",
        "\n",
        "    # (1) POS Tagging\n",
        "    pos_tags, noun_count, verb_count, adj_count, adv_count = pos_tagging_and_count(clean_text)\n",
        "    print(\"\\nPOS Tagging:\")\n",
        "    print(pos_tags)\n",
        "    print(f\"\\nNoun Count: {noun_count}, Verb Count: {verb_count}, Adjective Count: {adj_count}, Adverb Count: {adv_count}\")\n",
        "\n",
        "    # (2) Dependency Parsing and Constituency Parsing\n",
        "    print(\"\\nConstituency and Dependency Parsing:\")\n",
        "    parse_sentence(clean_text)\n",
        "\n",
        "    # (3) Named Entity Recognition (NER)\n",
        "    entities, entity_count = named_entity_recognition(clean_text)\n",
        "    print(\"\\nNamed Entity Recognition:\")\n",
        "    print(entities)\n",
        "    print(f\"Entity Counts: {entity_count}\")\n",
        "\n",
        "    # For demo purposes, we are analyzing only the first review\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45CBJTfr6J4Z",
        "outputId": "9e1be4bc-31a5-42b8-c712-efd36a8fd2df"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Review 1 ###\n",
            "Clean Text: baahubali continues epic silliness first baahubali even bigger battles dance scenes menacing looks muscles surprise reveal end part explained circle completed transition hero role amarendra baahubali son mahendra baahubali played prabhas infant saved murderous uncle queen seen prologue first film like antecedent film swings wildly almost slapstick comedy eg baahubali passes blockhead get near princess devasena bloody cartoonish violence including multiple impalements immolations decapitations buzzsaw chariot back equipped sort volleygun arrow launcher tactical highlight palmtree catapults launch teams soldiers use shields form armored balls midair even wile e coyote would dismissed strategy improbable laws physics outrageously ignored throughout action scenes particularly egregious fun example surprisingly film cgi good places times looking like videogame epic liveaction film watched subtitled version cant really comment acting leads still look sound part anushka shettys devasena sathyarajs kattappa particularly good weak scenes weird flying ship dream sequence comes mind imo kept overall good overthetop epic fantasy perhaps somewhat different aesthetic western audiences used picture aragorn arwen backed dancing elves breaking love duet middle council elrond\n",
            "\n",
            "POS Tagging:\n",
            "[('baahubali', 'NN'), ('continues', 'VBZ'), ('epic', 'CC'), ('silliness', 'NN'), ('first', 'RB'), ('baahubali', 'VBD'), ('even', 'RB'), ('bigger', 'JJR'), ('battles', 'NNS'), ('dance', 'NN'), ('scenes', 'NNS'), ('menacing', 'VBG'), ('looks', 'NNS'), ('muscles', 'NNS'), ('surprise', 'RB'), ('reveal', 'JJ'), ('end', 'VB'), ('part', 'NN'), ('explained', 'VBN'), ('circle', 'NN'), ('completed', 'VBN'), ('transition', 'NN'), ('hero', 'NN'), ('role', 'NN'), ('amarendra', 'NN'), ('baahubali', 'IN'), ('son', 'NN'), ('mahendra', 'NN'), ('baahubali', 'NN'), ('played', 'VBD'), ('prabhas', 'NN'), ('infant', 'NN'), ('saved', 'VBD'), ('murderous', 'JJ'), ('uncle', 'NN'), ('queen', 'NN'), ('seen', 'VBN'), ('prologue', 'NN'), ('first', 'RB'), ('film', 'NN'), ('like', 'IN'), ('antecedent', 'NN'), ('film', 'NN'), ('swings', 'NNS'), ('wildly', 'RB'), ('almost', 'RB'), ('slapstick', 'JJ'), ('comedy', 'NN'), ('eg', 'NN'), ('baahubali', 'NN'), ('passes', 'VBZ'), ('blockhead', 'JJ'), ('get', 'NN'), ('near', 'IN'), ('princess', 'JJ'), ('devasena', 'NN'), ('bloody', 'NN'), ('cartoonish', 'JJ'), ('violence', 'NN'), ('including', 'VBG'), ('multiple', 'JJ'), ('impalements', 'NNS'), ('immolations', 'NNS'), ('decapitations', 'NNS'), ('buzzsaw', 'VBD'), ('chariot', 'NN'), ('back', 'RB'), ('equipped', 'VBD'), ('sort', 'NN'), ('volleygun', 'NN'), ('arrow', 'NN'), ('launcher', 'RB'), ('tactical', 'JJ'), ('highlight', 'NN'), ('palmtree', 'NN'), ('catapults', 'NNS'), ('launch', 'JJ'), ('teams', 'NNS'), ('soldiers', 'NNS'), ('use', 'VBP'), ('shields', 'NNS'), ('form', 'RB'), ('armored', 'VBD'), ('balls', 'NNS'), ('midair', 'FW'), ('even', 'RB'), ('wile', 'VBP'), ('e', 'JJ'), ('coyote', 'NN'), ('would', 'MD'), ('dismissed', 'VB'), ('strategy', 'NN'), ('improbable', 'JJ'), ('laws', 'NNS'), ('physics', 'NNS'), ('outrageously', 'RB'), ('ignored', 'VBN'), ('throughout', 'IN'), ('action', 'NN'), ('scenes', 'NNS'), ('particularly', 'RB'), ('egregious', 'JJ'), ('fun', 'NN'), ('example', 'NN'), ('surprisingly', 'RB'), ('film', 'NN'), ('cgi', 'NN'), ('good', 'JJ'), ('places', 'NNS'), ('times', 'NNS'), ('looking', 'VBG'), ('like', 'IN'), ('videogame', 'NN'), ('epic', 'NN'), ('liveaction', 'NN'), ('film', 'NN'), ('watched', 'VBD'), ('subtitled', 'VBN'), ('version', 'NN'), ('cant', 'NN'), ('really', 'RB'), ('comment', 'JJ'), ('acting', 'VBG'), ('leads', 'NNS'), ('still', 'RB'), ('look', 'VBP'), ('sound', 'JJ'), ('part', 'NN'), ('anushka', 'NN'), ('shettys', 'NN'), ('devasena', 'NN'), ('sathyarajs', 'JJ'), ('kattappa', 'NN'), ('particularly', 'RB'), ('good', 'JJ'), ('weak', 'JJ'), ('scenes', 'NNS'), ('weird', 'IN'), ('flying', 'VBG'), ('ship', 'JJ'), ('dream', 'NN'), ('sequence', 'NN'), ('comes', 'VBZ'), ('mind', 'RB'), ('imo', 'JJ'), ('kept', 'VBD'), ('overall', 'JJ'), ('good', 'JJ'), ('overthetop', 'NN'), ('epic', 'NN'), ('fantasy', 'NN'), ('perhaps', 'RB'), ('somewhat', 'RB'), ('different', 'JJ'), ('aesthetic', 'JJ'), ('western', 'JJ'), ('audiences', 'NNS'), ('used', 'VBN'), ('picture', 'NN'), ('aragorn', 'JJ'), ('arwen', 'NN'), ('backed', 'VBD'), ('dancing', 'VBG'), ('elves', 'NNS'), ('breaking', 'VBG'), ('love', 'NN'), ('duet', 'NN'), ('middle', 'JJ'), ('council', 'NN'), ('elrond', 'NN')]\n",
            "\n",
            "Noun Count: 84, Verb Count: 30, Adjective Count: 28, Adverb Count: 19\n",
            "\n",
            "Constituency and Dependency Parsing:\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "baahubali -> nsubj (Head: continues)\n",
            "continues -> nsubj (Head: saved)\n",
            "epic -> compound (Head: silliness)\n",
            "silliness -> dobj (Head: continues)\n",
            "first -> advmod (Head: baahubali)\n",
            "baahubali -> ccomp (Head: continues)\n",
            "even -> advmod (Head: bigger)\n",
            "bigger -> amod (Head: battles)\n",
            "battles -> compound (Head: scenes)\n",
            "dance -> compound (Head: scenes)\n",
            "scenes -> dobj (Head: baahubali)\n",
            "menacing -> advcl (Head: baahubali)\n",
            "looks -> advcl (Head: continues)\n",
            "muscles -> nsubj (Head: surprise)\n",
            "surprise -> compound (Head: part)\n",
            "reveal -> amod (Head: end)\n",
            "end -> compound (Head: part)\n",
            "part -> nsubj (Head: explained)\n",
            "explained -> conj (Head: continues)\n",
            "circle -> nmod (Head: role)\n",
            "completed -> amod (Head: role)\n",
            "transition -> compound (Head: hero)\n",
            "hero -> compound (Head: role)\n",
            "role -> nsubj (Head: played)\n",
            "amarendra -> compound (Head: son)\n",
            "baahubali -> compound (Head: son)\n",
            "son -> compound (Head: baahubali)\n",
            "mahendra -> compound (Head: baahubali)\n",
            "baahubali -> nsubj (Head: played)\n",
            "played -> ccomp (Head: explained)\n",
            "prabhas -> compound (Head: infant)\n",
            "infant -> dobj (Head: played)\n",
            "saved -> ROOT (Head: saved)\n",
            "murderous -> amod (Head: queen)\n",
            "uncle -> compound (Head: queen)\n",
            "queen -> nsubj (Head: launch)\n",
            "seen -> acl (Head: queen)\n",
            "prologue -> nmod (Head: film)\n",
            "first -> amod (Head: film)\n",
            "film -> dobj (Head: seen)\n",
            "like -> prep (Head: seen)\n",
            "antecedent -> amod (Head: swings)\n",
            "film -> compound (Head: swings)\n",
            "swings -> nsubj (Head: get)\n",
            "wildly -> advmod (Head: passes)\n",
            "almost -> advmod (Head: slapstick)\n",
            "slapstick -> amod (Head: passes)\n",
            "comedy -> compound (Head: baahubali)\n",
            "eg -> compound (Head: baahubali)\n",
            "baahubali -> compound (Head: passes)\n",
            "passes -> compound (Head: blockhead)\n",
            "blockhead -> nsubj (Head: get)\n",
            "get -> pcomp (Head: like)\n",
            "near -> prep (Head: get)\n",
            "princess -> pobj (Head: near)\n",
            "devasena -> nmod (Head: violence)\n",
            "bloody -> amod (Head: violence)\n",
            "cartoonish -> amod (Head: violence)\n",
            "violence -> dobj (Head: get)\n",
            "including -> prep (Head: violence)\n",
            "multiple -> amod (Head: impalements)\n",
            "impalements -> pobj (Head: including)\n",
            "immolations -> compound (Head: decapitations)\n",
            "decapitations -> appos (Head: impalements)\n",
            "buzzsaw -> compound (Head: chariot)\n",
            "chariot -> appos (Head: impalements)\n",
            "back -> advmod (Head: chariot)\n",
            "equipped -> amod (Head: impalements)\n",
            "sort -> advmod (Head: volleygun)\n",
            "volleygun -> amod (Head: catapults)\n",
            "arrow -> advmod (Head: volleygun)\n",
            "launcher -> nmod (Head: palmtree)\n",
            "tactical -> amod (Head: palmtree)\n",
            "highlight -> compound (Head: palmtree)\n",
            "palmtree -> compound (Head: catapults)\n",
            "catapults -> nsubj (Head: launch)\n",
            "launch -> ccomp (Head: saved)\n",
            "teams -> compound (Head: soldiers)\n",
            "soldiers -> nsubj (Head: use)\n",
            "use -> ccomp (Head: launch)\n",
            "shields -> dobj (Head: use)\n",
            "form -> xcomp (Head: use)\n",
            "armored -> amod (Head: balls)\n",
            "balls -> dobj (Head: form)\n",
            "midair -> conj (Head: launch)\n",
            "even -> advmod (Head: wile)\n",
            "wile -> compound (Head: coyote)\n",
            "e -> compound (Head: coyote)\n",
            "coyote -> dobj (Head: midair)\n",
            "would -> aux (Head: dismissed)\n",
            "dismissed -> conj (Head: saved)\n",
            "strategy -> dobj (Head: dismissed)\n",
            "improbable -> amod (Head: physics)\n",
            "laws -> compound (Head: physics)\n",
            "physics -> nsubj (Head: ignored)\n",
            "outrageously -> advmod (Head: ignored)\n",
            "ignored -> relcl (Head: strategy)\n",
            "throughout -> prep (Head: ignored)\n",
            "action -> compound (Head: scenes)\n",
            "scenes -> pobj (Head: throughout)\n",
            "particularly -> advmod (Head: egregious)\n",
            "egregious -> amod (Head: example)\n",
            "fun -> amod (Head: example)\n",
            "example -> nsubj (Head: film)\n",
            "surprisingly -> advmod (Head: film)\n",
            "film -> advcl (Head: dismissed)\n",
            "cgi -> nmod (Head: places)\n",
            "good -> amod (Head: places)\n",
            "places -> dobj (Head: film)\n",
            "times -> prep (Head: dismissed)\n",
            "looking -> advcl (Head: dismissed)\n",
            "like -> prep (Head: looking)\n",
            "videogame -> nmod (Head: film)\n",
            "epic -> amod (Head: film)\n",
            "liveaction -> compound (Head: film)\n",
            "film -> pobj (Head: like)\n",
            "watched -> advcl (Head: dismissed)\n",
            "subtitled -> amod (Head: version)\n",
            "version -> nsubj (Head: comment)\n",
            "ca -> aux (Head: comment)\n",
            "nt -> neg (Head: comment)\n",
            "really -> advmod (Head: comment)\n",
            "comment -> ccomp (Head: watched)\n",
            "acting -> compound (Head: leads)\n",
            "leads -> dobj (Head: comment)\n",
            "still -> advmod (Head: look)\n",
            "look -> conj (Head: saved)\n",
            "sound -> amod (Head: anushka)\n",
            "part -> compound (Head: anushka)\n",
            "anushka -> compound (Head: kattappa)\n",
            "shettys -> compound (Head: kattappa)\n",
            "devasena -> compound (Head: kattappa)\n",
            "sathyarajs -> compound (Head: kattappa)\n",
            "kattappa -> nmod (Head: sequence)\n",
            "particularly -> advmod (Head: good)\n",
            "good -> amod (Head: scenes)\n",
            "weak -> amod (Head: scenes)\n",
            "scenes -> nmod (Head: sequence)\n",
            "weird -> amod (Head: sequence)\n",
            "flying -> amod (Head: ship)\n",
            "ship -> compound (Head: dream)\n",
            "dream -> compound (Head: sequence)\n",
            "sequence -> nsubj (Head: comes)\n",
            "comes -> conj (Head: saved)\n",
            "mind -> npadvmod (Head: comes)\n",
            "imo -> advmod (Head: kept)\n",
            "kept -> conj (Head: saved)\n",
            "overall -> amod (Head: overthetop)\n",
            "good -> amod (Head: overthetop)\n",
            "overthetop -> nmod (Head: fantasy)\n",
            "epic -> amod (Head: fantasy)\n",
            "fantasy -> dobj (Head: kept)\n",
            "perhaps -> advmod (Head: different)\n",
            "somewhat -> advmod (Head: different)\n",
            "different -> amod (Head: audiences)\n",
            "aesthetic -> amod (Head: audiences)\n",
            "western -> amod (Head: audiences)\n",
            "audiences -> nsubj (Head: used)\n",
            "used -> ccomp (Head: kept)\n",
            "picture -> dobj (Head: used)\n",
            "aragorn -> compound (Head: arwen)\n",
            "arwen -> npadvmod (Head: backed)\n",
            "backed -> amod (Head: dancing)\n",
            "dancing -> nsubj (Head: elves)\n",
            "elves -> ccomp (Head: kept)\n",
            "breaking -> xcomp (Head: elves)\n",
            "love -> compound (Head: duet)\n",
            "duet -> dobj (Head: breaking)\n",
            "middle -> compound (Head: council)\n",
            "council -> compound (Head: elrond)\n",
            "elrond -> oprd (Head: kept)\n",
            "\n",
            "Named Entity Recognition:\n",
            "[('amarendra baahubali', 'PERSON'), ('mahendra baahubali', 'PERSON'), ('first', 'ORDINAL')]\n",
            "Entity Counts: Counter({'PERSON': 2, 'ORDINAL': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'/content/combined_imdb_reviews_cleaned.csv'"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "'''\n",
        "I found it quite interesting to do this assignment while scraping. I enjoyed when I cleaned and downloaded the csv file. I found it challengin to write the code.\n",
        "we need more time for these assignments'''"
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1c9af5c4-6daf-4b85-d7bd-7450244a534c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nI found it quite interesting to do this assignment while scraping. I enjoyed when I cleaned and downloaded the csv file. I found it challengin to write the code.\\nwe need more time for these assignments'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sGtS4N95DW28"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}